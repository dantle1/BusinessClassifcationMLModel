{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy import random as npr\n",
    "import time\n",
    "import string\n",
    "import sklearn\n",
    "import random\n",
    "# from itertools import chain, imap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dhmjoe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dhmjoe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download stopwords and punctuations \n",
    "nltk.download('punkt') \n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from operator import itemgetter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data\n",
    "df1 = pd.read_json('/Users/dhmjoe/Desktop/carpe data Haoming/final_50k_221118.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select first 50000 rows\n",
    "first10 = df1[0:450000]\n",
    "\n",
    "#Extract business name and content from the first 50000 rows\n",
    "df2 = first10[[\"business_name\",\"content\"]]\n",
    "\n",
    "# Create a seperate data frame for labels\n",
    "label = [\"is_entertainment\",\"is_traffic\"]\n",
    "\n",
    "#Extract labels from the first 50000 rows\n",
    "labeldf = first10[[\"is_entertainment\",\"is_traffic\"]]\n",
    "\n",
    "labeldf_1 = labeldf[\"is_entertainment\"]\n",
    "labeldf_2 = labeldf[\"is_traffic\"]\n",
    "# np.unique(labeldf_2)\n",
    "np.unique(labeldf_1)\n",
    "#print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = []\n",
    "name = [\"business_name\",\"content\"]\n",
    "dic1 = {}\n",
    "WORD = re.compile(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'\\w+', re.UNICODE)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regTokenize(text):\n",
    "    words = WORD.findall(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1]:\n",
    "    arrx = []\n",
    "    for j in range(0,450000):\n",
    "    #for j in range(0,len(df2[name[i]])):\n",
    "        listx = regTokenize(df2[name[i]][j])\n",
    "        arrx.append(listx)\n",
    "    dic1[name[i]] = arrx\n",
    "# print(dic1)\n",
    "# dic1 contains all the tokens of business name and contents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'|', 'yourself', 'she', 'wasn', '\"', 'off', \"couldn't\", 'until', \"you're\", '#', \"hasn't\", 'myself', 'up', 'weren', 'a', 'should', '&', 'they', 'your', 'both', 'yourselves', 'do', \"mightn't\", 'with', 'o', \"you'd\", \"hadn't\", 'yours', '.', 'any', 'an', 'can', 'don', 'most', 'once', 'wouldn', 'will', 'we', '(', 'ours', \"it's\", 'them', 'these', 'between', 'during', 'ma', ')', \"you've\", 't', 'those', 'their', 'i', 'through', 'couldn', 'does', 'then', '{', 'against', 'in', 'down', 'few', 'each', 'under', 'himself', 'mustn', \"that'll\", '<', '~', 'hers', ';', 'to', 'all', 'aren', 'now', 'more', 'our', '`', 'very', '$', '*', 'same', 'him', 'didn', 'on', '!', 'no', 'herself', 'if', 'just', 'or', 'after', '-', '_', 'because', 'had', 'having', 'm', 'my', \"'s\", 'what', 'over', \"aren't\", 've', 'haven', 'whom', '[', 'such', 'nor', 'hasn', 'shan', 'so', 'did', 'the', \"you'll\", \"didn't\", 'from', \"wasn't\", 'ourselves', 'at', 'll', \"won't\", 'as', 'own', 'this', 'out', 'which', 'again', 'is', \"haven't\", '^', 'has', 'by', 'theirs', 'he', 'and', 'not', \"'\", 'hadn', \"weren't\", 'his', 'you', 's', 'for', \"she's\", 'its', \"shan't\", '>', 'itself', 'there', 'here', 'was', 'how', '?', 'themselves', 'only', \"wouldn't\", 'it', '%', 'needn', '@', 'shouldn', 'that', 'when', 'than', 'are', \"isn't\", 'doing', 'about', 'where', '=', '}', 'me', 'her', ':', 'why', 'further', 'mightn', \"should've\", \"doesn't\", \"shouldn't\", 'of', 'doesn', 're', 'isn', 'before', 'd', 'other', 'being', '.....', 'into', 'above', 'ain', \"needn't\", 'have', '+', ']', 'too', 'y', ',', '\\\\', 'were', 'won', 'who', 'be', 'while', \"don't\", '/', 'below', 'some', \"mustn't\", 'but', 'am', 'been'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "for i in range(len(string.punctuation)):    \n",
    "    stop_words.add(string.punctuation[i])\n",
    "stop_words.add(\"'s\")\n",
    "stop_words.add(\".....\")\n",
    "print(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "# print(dic1)\n",
    "for i in [0,1]:\n",
    "    arrc = []\n",
    "    for j in range(0,len(dic1[name[i]])):\n",
    "        flist = []\n",
    "        for z in dic1[name[i]][j]:\n",
    "            if z.casefold() not in stop_words:\n",
    "                flist.append(stemmer.stem(z))\n",
    "        arrc.append(' '.join(flist))\n",
    "    dic1[name[i]] = arrc\n",
    "    #print(len(dic1[name[i]]))\n",
    "    \n",
    "# print(dic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         76  ace  aid  american  ampm  angel  antiqu  applianc  arco  art  \\\n",
      "0       0.0  0.0  0.0       0.0   0.0    0.0     0.0       0.0   0.0  0.0   \n",
      "1       0.0  0.0  0.0       0.0   0.0    0.0     0.0       0.0   0.0  0.0   \n",
      "2       0.0  0.0  0.0       0.0   0.0    0.0     0.0       0.0   0.0  0.0   \n",
      "3       0.0  0.0  0.0       0.0   0.0    0.0     0.0       0.0   0.0  0.0   \n",
      "4       0.0  0.0  0.0       0.0   0.0    0.0     0.0       0.0   0.0  0.0   \n",
      "...     ...  ...  ...       ...   ...    ...     ...       ...   ...  ...   \n",
      "449995  0.0  0.0  0.0       0.0   0.0    0.0     0.0       0.0   0.0  0.0   \n",
      "449996  0.0  0.0  0.0       0.0   0.0    0.0     0.0       0.0   0.0  0.0   \n",
      "449997  0.0  0.0  0.0       0.0   0.0    0.0     0.0       0.0   0.0  0.0   \n",
      "449998  0.0  0.0  0.0       0.0   0.0    0.0     0.0       0.0   0.0  0.0   \n",
      "449999  0.0  0.0  0.0       0.0   0.0    0.0     0.0       0.0   0.0  0.0   \n",
      "\n",
      "        ...  west  wholesal  wild  wine  wineri  wing  world  yelp  yogurt  \\\n",
      "0       ...   0.0       0.0   0.0   0.0     0.0   0.0    0.0   0.0     0.0   \n",
      "1       ...   0.0       0.0   0.0   0.0     0.0   0.0    0.0   0.0     0.0   \n",
      "2       ...   0.0       0.0   0.0   0.0     0.0   0.0    0.0   0.0     0.0   \n",
      "3       ...   0.0       0.0   0.0   0.0     0.0   0.0    0.0   0.0     0.0   \n",
      "4       ...   0.0       0.0   0.0   0.0     0.0   0.0    0.0   0.0     0.0   \n",
      "...     ...   ...       ...   ...   ...     ...   ...    ...   ...     ...   \n",
      "449995  ...   0.0       0.0   0.0   0.0     0.0   0.0    0.0   0.0     0.0   \n",
      "449996  ...   0.0       0.0   0.0   0.0     0.0   0.0    0.0   0.0     0.0   \n",
      "449997  ...   0.0       0.0   0.0   0.0     0.0   0.0    0.0   0.0     0.0   \n",
      "449998  ...   0.0       0.0   0.0   0.0     0.0   0.0    0.0   0.0     0.0   \n",
      "449999  ...   0.0       0.0   0.0   0.0     0.0   0.0    0.0   0.0     0.0   \n",
      "\n",
      "        zone  \n",
      "0        0.0  \n",
      "1        0.0  \n",
      "2        0.0  \n",
      "3        0.0  \n",
      "4        0.0  \n",
      "...      ...  \n",
      "449995   0.0  \n",
      "449996   0.0  \n",
      "449997   0.0  \n",
      "449998   0.0  \n",
      "449999   0.0  \n",
      "\n",
      "[450000 rows x 372 columns]\n",
      "         00  000        10  100  10pm   11   12   13   14        15  ...  yet  \\\n",
      "0       0.0  0.0  0.055454  0.0   0.0  0.0  0.0  0.0  0.0  0.066550  ...  0.0   \n",
      "1       0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0   \n",
      "2       0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0   \n",
      "3       0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0   \n",
      "4       0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0   \n",
      "...     ...  ...       ...  ...   ...  ...  ...  ...  ...       ...  ...  ...   \n",
      "449995  0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  0.082875  ...  0.0   \n",
      "449996  0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0   \n",
      "449997  0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0   \n",
      "449998  0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0   \n",
      "449999  0.0  0.0  0.000000  0.0   0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0   \n",
      "\n",
      "         yo  yogurt  york     young  younger   yr  yum  yummi  zero  \n",
      "0       0.0     0.0   0.0  0.000000      0.0  0.0  0.0    0.0   0.0  \n",
      "1       0.0     0.0   0.0  0.000000      0.0  0.0  0.0    0.0   0.0  \n",
      "2       0.0     0.0   0.0  0.000000      0.0  0.0  0.0    0.0   0.0  \n",
      "3       0.0     0.0   0.0  0.000000      0.0  0.0  0.0    0.0   0.0  \n",
      "4       0.0     0.0   0.0  0.000000      0.0  0.0  0.0    0.0   0.0  \n",
      "...     ...     ...   ...       ...      ...  ...  ...    ...   ...  \n",
      "449995  0.0     0.0   0.0  0.090851      0.0  0.0  0.0    0.0   0.0  \n",
      "449996  0.0     0.0   0.0  0.000000      0.0  0.0  0.0    0.0   0.0  \n",
      "449997  0.0     0.0   0.0  0.000000      0.0  0.0  0.0    0.0   0.0  \n",
      "449998  0.0     0.0   0.0  0.000000      0.0  0.0  0.0    0.0   0.0  \n",
      "449999  0.0     0.0   0.0  0.000000      0.0  0.0  0.0    0.0   0.0  \n",
      "\n",
      "[450000 rows x 2534 columns]\n"
     ]
    }
   ],
   "source": [
    "tflist = []\n",
    "for i in [0,1]:\n",
    "    tr_idf_model  = TfidfVectorizer(min_df=0.001)\n",
    "    corpus = dic1[name[i]]\n",
    "    tf_idf_vector = tr_idf_model.fit_transform(corpus)\n",
    "    tf_idf_array = tf_idf_vector.toarray()\n",
    "    words_set = tr_idf_model.get_feature_names_out()\n",
    "    #print(words_set)\n",
    "    df_tf_idf= pd.DataFrame(tf_idf_array, columns = words_set)\n",
    "    tflist.append(df_tf_idf)\n",
    "    #This is the tf-idf result that we can use for SVM or Simple Bayes\n",
    "    print(df_tf_idf)\n",
    "    # print(labeldf[label[i]])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #tfidf = []\n",
    "    tfidf = [(j,sum(df_tf_idf[j])) for j in words_set]\n",
    "    tfidf.sort(key=lambda x: x[1],reverse=True)\n",
    "    print(name[i],tfidf)\n",
    "    end = time.time()\n",
    "    print(name[i],end-now)\n",
    "    now = end'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_name is_entertainment\n",
      "Accuracy: 0.92\n",
      "[[76966   615]\n",
      " [ 6461  5958]] : is the confusion matrix\n",
      "0.9064354176175262 : is the precision score\n",
      "0.4797487720428376 : is the recall score\n",
      "0.6274220724515585 : is the f1 score\n",
      "business_name is_traffic\n",
      "Accuracy: 0.87\n",
      "[[ 2347 10119]\n",
      " [ 1425 76109]] : is the confusion matrix\n",
      "0.8826483276893816 : is the precision score\n",
      "0.9816209662857585 : is the recall score\n",
      "0.92950745594216 : is the f1 score\n",
      "content is_entertainment\n",
      "Accuracy: 0.89\n",
      "[[77378   203]\n",
      " [ 9976  2443]] : is the confusion matrix\n",
      "0.9232804232804233 : is the precision score\n",
      "0.1967147113294146 : is the recall score\n",
      "0.32432791237968805 : is the f1 score\n",
      "content is_traffic\n",
      "Accuracy: 0.86\n",
      "[[  333 12133]\n",
      " [  342 77192]] : is the confusion matrix\n",
      "0.864170165127344 : is the precision score\n",
      "0.9955890319085821 : is the recall score\n",
      "0.9252362773359543 : is the f1 score\n"
     ]
    }
   ],
   "source": [
    "npr.seed(123)\n",
    "for i in [0,1]:\n",
    "    for j in [0,1]:\n",
    "        print(name[i],label[j])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(tflist[i], labeldf[label[j]], test_size=0.2, random_state=434)\n",
    "        # create and fit the Naive Bayes model\n",
    "        nb_model = MultinomialNB()\n",
    "        #nb_model = GaussianNB()\n",
    "        nb_model.fit(X_train, y_train)\n",
    "\n",
    "        # make predictions on the test data\n",
    "        y_pred = nb_model.predict(X_test)\n",
    "        \n",
    "        '''\n",
    "        param_grid_nb = {\n",
    "        'var_smoothing': np.logspace(0,-9, num=100)\n",
    "        }\n",
    "        nbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)\n",
    "        nbModel_grid.fit(X_train, y_train)\n",
    "        print(nbModel_grid.best_estimator_)\n",
    "        y_pred = nbModel_grid.predict(X_test)\n",
    "        '''\n",
    "\n",
    "        # calculate accuracy\n",
    "        #accuracy = (y_pred == y_test).sum() / len(y_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "        print(confusion_matrix(y_test, y_pred), \": is the confusion matrix\")\n",
    "\n",
    "        print(precision_score(y_test, y_pred), \": is the precision score\")\n",
    "\n",
    "        print(recall_score(y_test, y_pred), \": is the recall score\")\n",
    "\n",
    "        print(f1_score(y_test, y_pred), \": is the f1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "# perform encoding on training and testing dataset to differentiate between different labels and assign them to 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Encoder = LabelEncoder()\n",
    "y_train_svm = Encoder.fit_transform(y_train)\n",
    "y_test_svm = Encoder.fit_transform(y_test)\n",
    "SVM = svm.SVC(C=1.0, kernel = 'linear', degree = 3, gamma = 'auto')\n",
    "SVM.fit(X_train,y_train_svm)\n",
    "predictions_SVM = SVM.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(predictions_SVM, y_test_svm)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
